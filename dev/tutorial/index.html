<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · ImplicitAD.jl</title><meta name="title" content="Tutorial · ImplicitAD.jl"/><meta property="og:title" content="Tutorial · ImplicitAD.jl"/><meta property="twitter:title" content="Tutorial · ImplicitAD.jl"/><meta name="description" content="Documentation for ImplicitAD.jl."/><meta property="og:description" content="Documentation for ImplicitAD.jl."/><meta property="twitter:description" content="Documentation for ImplicitAD.jl."/><meta property="og:url" content="https://byuflowlab.github.io/ImplicitAD.jl/tutorial/"/><meta property="twitter:url" content="https://byuflowlab.github.io/ImplicitAD.jl/tutorial/"/><link rel="canonical" href="https://byuflowlab.github.io/ImplicitAD.jl/tutorial/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ImplicitAD.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Basic-Usage"><span>Basic Usage</span></a></li><li><a class="tocitem" href="#Overloading-Subfunctions"><span>Overloading Subfunctions</span></a></li><li><a class="tocitem" href="#Linear-residuals"><span>Linear residuals</span></a></li><li><a class="tocitem" href="#Eigenvalue-Problems"><span>Eigenvalue Problems</span></a></li><li><a class="tocitem" href="#Ordinary-Differential-Equations"><span>Ordinary Differential Equations</span></a></li><li><a class="tocitem" href="#Custom-Rules-(e.g.,-calling-Python-or-other-external-code-packages)"><span>Custom Rules (e.g., calling Python or other external code packages)</span></a></li><li><a class="tocitem" href="#Using-Julia-AD-to-Provide-Derivatives-in-Python"><span>Using Julia AD to Provide Derivatives in Python</span></a></li></ul></li><li><a class="tocitem" href="../reference/">API</a></li><li><a class="tocitem" href="../theory/">Theory</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/byuflowlab/ImplicitAD.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/byuflowlab/ImplicitAD.jl/blob/main/docs/src/tutorial.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Quick-Start"><a class="docs-heading-anchor" href="#Quick-Start">Quick Start</a><a id="Quick-Start-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Start" title="Permalink"></a></h1><p>We can generically represent the solver that converges the residuals and computes the corresponding state variables as:</p><p><code>y = solve(x, p)</code></p><p>Where x are variables and p are fixed parameters.  Our larger code may then have a mix of explicit and implicit functions.</p><pre><code class="language-julia hljs">function example(a)
    b = 2*a
    x = @. exp(b) + a
    y = solve(x)
    z = sqrt.(y)
    return z
end</code></pre><p>To make this function compatible we only need to replace the call to <code>solve</code> with an overloaded function <code>implicit</code> defined in this module:</p><pre><code class="language-julia hljs">using ImplicitAD

function example(a)
    b = 2*a
    x = @. exp(b) + a
    y = implicit(solve, residual, x)
    z = sqrt.(y)
    return z
end</code></pre><p>Note that the only new piece of information we need to expose is the residual function so that partial derivatives can be computed.  The new function is now compatible with ForwardDiff or ReverseDiff, for any solver, and efficiently provides the correct derivatives without differentiating inside the solver.</p><p>The residuals can either be returned: <code>r = residual(y, x, p)</code> or modified in place: <code>residual!(r, y, x, p)</code>.</p><p>The input x should be a vector, and p is a tuple of fixed parameters.  The state and corresponding residuals, y and r, can be vectors or scalars (for 1D residuals).  The subfunctions are overloaded to handle both cases efficiently.</p><p>Limitation: ReverseDiff does not currently support compiling the tape for custome rules.  See this issue in ReverseDiff: <a href="https://github.com/JuliaDiff/ReverseDiff.jl/issues/187">https://github.com/JuliaDiff/ReverseDiff.jl/issues/187</a></p><h2 id="Basic-Usage"><a class="docs-heading-anchor" href="#Basic-Usage">Basic Usage</a><a id="Basic-Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Usage" title="Permalink"></a></h2><p>Let&#39;s go through a complete example now. Assume we have two nonlinear implicit equations:</p><p class="math-container">\[r_1(x, y) = (y_1 + x_1) (y_2^3 - x_2) + x_3 = 0
r_2(x, y) = \sin(y_2 \exp(y_1) - 1) x_4 = 0\]</p><p>We will use the NLsolve package to solve these equations (refer to the first example in their documentation if not familiar with NLsolve).  We will also put explict operations before and after the solve just to show how this will work in the midst of a larger program.  In this case we use the in-place form of the residual function.</p><pre><code class="language-julia hljs">using NLsolve

function residual!(r, y, x, p)
    r[1] = (y[1] + x[1])*(y[2]^3-x[2])+x[3]
    r[2] = sin(y[2]*exp(y[1])-1)*x[4]
end

function solve(x, p)
    rwrap(r, y) = residual!(r, y, x[1:4], p)  # closure using some of the input variables within x just as an example
    res = nlsolve(rwrap, [0.1; 1.2], autodiff=:forward)
    return res.zero
end

function program(x)
    z = 2.0*x
    w = z + x.^2
    y = solve(w)
    return y[1] .+ w*y[2]
end</code></pre><p>Now if we tried to run <code>ForwardDiff.jacobian(program, x)</code> it will not work (actually it will work if we change the type of our starting point <span>$x_0$</span> to also be a Dual).  But even for solvers where we can propagate AD through it would typically be an inefficient way to compute the derivatives.  We now need to modify this script to use our package.  Here is what the modified <code>program</code> function will look like.</p><pre><code class="language-julia hljs">using ImplicitAD

function modprogram(x)
    z = 2.0*x
    w = z + x.^2
    y = implicit(solve, residual!, w)
    return y[1] .+ w*y[2]
end</code></pre><p>It is now both ForwardDiff and ReverseDiff compatible.</p><pre><code class="language-julia hljs">using ForwardDiff
using ReverseDiff

x = [1.0; 2.0; 3.0; 4.0; 5.0]

J1 = ForwardDiff.jacobian(modprogram, x)
J2 = ReverseDiff.jacobian(modprogram, x)
println(J1)
println(&quot;max abs difference = &quot;, maximum(abs.(J1 - J2)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[8.05246842094794 1.94270623081742 -0.9587895057912197 -1.5408316665440452e-16 0.0; 8.555718860266143 14.330684545988305 -3.0818950100571225 -2.9040933686224584e-16 0.0; 16.807320957115106 12.267150376875744 4.727259400832726 -4.812659751532237e-16 0.0; 27.416523653063773 20.010483486419485 -9.875832623708009 13.476877646077641 0.0; 40.383326948112135 29.47455728697295 -14.546664733092996 -1.026570655984589e-15 16.172253175293168]
max abs difference = 7.105427357601002e-15</code></pre><h2 id="Overloading-Subfunctions"><a class="docs-heading-anchor" href="#Overloading-Subfunctions">Overloading Subfunctions</a><a id="Overloading-Subfunctions-1"></a><a class="docs-heading-anchor-permalink" href="#Overloading-Subfunctions" title="Permalink"></a></h2><p>If the user can provide (or lazily compute) their own partial derivatives for ∂r/∂y then they can provide their own subfunction: <code>∂r∂y = drdy(residual, y, x, p)</code> (where <code>r = residual(y, x, p)</code>).  The default implementation computes these partials with ForwardDiff. Additionally the user can override the linear solve: <code>x = lsolve(A, b)</code>.  The default is the backslash operator.</p><p>Some examples where one may wish to override these behaviors are for cases with significant sparsity (e.g., using SparseDiffTools), to preallocate the Jacobian, to provide a specific matrix factorization, or if the number of states is large overriding both methods will often be beneficial so that you can use iterative linear solvers (matrix-free Krylov methods) and thus provide efficient Jacobian vector products rather than a Jacobian.</p><p>The other partials, ∂r/∂x, are not computed directly, but rather are used in efficient Jacobian vector (or vector Jacobian) products.</p><p>As an example, let&#39;s continue the same problem from the previous section.  We note that we can provide the Jacobian ∂r/∂y analytically and so we will skip the internal ForwardDiff implementation. We provide our own function for <code>drdy</code>, and we will preallocate so we can modify the Jacobian in place:</p><pre><code class="language-julia hljs">function drdy(residual, y, x, p, A)
    A[1, 1] = y[2]^3-x[2]
    A[1, 2] = 3*y[2]^2*(y[1]+x[1])
    u = exp(y[1])*cos(y[2]*exp(y[1])-1)*x[4]
    A[2, 1] = y[2]*u
    A[2, 2] = u
    return A
end</code></pre><p>We can now pass this function in with a keyword argument to replace the default implementation for this subfunction.</p><pre><code class="language-julia hljs">function modprogram2(x)
    z = 2.0*x
    w = z + x.^2
    A = zeros(2, 2)
    my_drdy(residual, y, x, p) = drdy(residual, y, x, p, A)
    p = () # no parameters in this case
    y = implicit(solve, residual!, w, p, drdy=my_drdy)
    return y[1] .+ w*y[2]
end

J3 = ForwardDiff.jacobian(modprogram2, x)
println(maximum(abs.(J1 - J3)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0</code></pre><h2 id="Linear-residuals"><a class="docs-heading-anchor" href="#Linear-residuals">Linear residuals</a><a id="Linear-residuals-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-residuals" title="Permalink"></a></h2><p>If the residuals are linear (i.e., Ay = b) we could still use the above nonlinear formulation but it will be inefficient or require more work from the user.  Instead, we can provide the partial derivatives analytically for the user.  In this case, the user need only provide the inputs A and b.</p><p>Let&#39;s consider a simple example.</p><pre><code class="language-julia hljs">using SparseArrays: sparse

function program(x)
    Araw = [x[1]*x[2] x[3]+x[4];
        x[3]+x[4] 0.0]
    b = [2.0, 3.0]
    A = sparse(Araw)
    y = A \ b
    z = y.^2
    return z
end</code></pre><p>This function is actually not compatible with ForwardDiff because of the use of a sparse matrix (obviously unnecessary with such a small matrix, just for illustration).  We now modify this function using this package, with a one line change using <code>implicit_linear</code>, and can now compute the Jacobian.</p><p>Let&#39;s consider a simple example.</p><pre><code class="language-julia hljs">using ImplicitAD
using ForwardDiff
using ReverseDiff

function modprogram(x)
    Araw = [x[1]*x[2] x[3]+x[4];
        x[3]+x[4] 0.0]
    b = [2.0, 3.0]
    A = sparse(Araw)
    y = implicit_linear(A, b)
    z = y.^2
    return z
end

x = [1.0; 2.0; 3.0; 4.0]

J1 = ForwardDiff.jacobian(modprogram, x)
J2 = ReverseDiff.jacobian(modprogram, x)

println(J1)
println(maximum(abs.(J1 - J2)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[-4.758098676964957e-17 -2.3790493384824785e-17 -0.052478134110787174 -0.052478134110787174; -0.03998334027488546 -0.01999167013744273 -0.0019039685845183543 -0.0019039685845183543]
6.938893903907228e-18</code></pre><p>For <code>implicit_linear</code> there are two keywords for custom subfunctions:</p><ol><li><code>lsolve(A, b)</code>: same purpose as before: solve <span>$A x = b$</span> where the default is the backslash operator.</li><li><code>fact(A)</code>: provide a matrix factorization of <span>$A$</span>, since two linear solves are performed (for the primal and dual values).  default is <code>factorize</code> defined in <code>LinearAlgebra</code>.</li></ol><h2 id="Eigenvalue-Problems"><a class="docs-heading-anchor" href="#Eigenvalue-Problems">Eigenvalue Problems</a><a id="Eigenvalue-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#Eigenvalue-Problems" title="Permalink"></a></h2><p>Like the linear case, we can provide analytic derivatives for eigenvalue problems (many of which are not overridden for AD anyway).  These are problems of the form:</p><p class="math-container">\[A v = \lambda B v\]</p><p>For standard eigenvalue problems B is the identity matrix.  The user just needs to provide the matrices A, B, and some function to solve the eigenvalue problem (which could use any method).  The solver should be in the following form: <code>λ, V, U = eigsolve(A, B)</code> where λ is a vector of eigenvalues, V is a matrix with corresponding eigenvectors in the columns (i.e., λ[i] corresponds to V[:, i]), and U is a matrix whose columns contain the left eigenvectors (u^H A = λ u^H B).  The left eigenvectors must be in the same order as the right eigenvectors (i.e., U&#39; * B * V must be diagonal).  U need not be normalized as we do that internally.  Note that if A and B are symmetric/Hermitian then U = V.  Currently only eigenvalue derivatives are provided (not eigenvector derivatives). Let&#39;s now see an example.</p><pre><code class="language-julia hljs">using ImplicitAD
using ForwardDiff
using ReverseDiff
using LinearAlgebra: eigvals, eigvecs

function eigsolve(A, B)
    λ = eigvals(A, B)
    V = eigvecs(A, B)
    U = eigvecs(A&#39;, B&#39;)

    return λ, V, U
end

function test(x)
    A = [x[1] x[2]; x[3] x[4]]
    B = [x[5] x[6]; x[7] x[8]]
    λ = ImplicitAD.implicit_eigval(A, B, eigsolve)  # replaced from λ, _, _ = eigsolve(A, B)
    z = [real(λ[1]) + imag(λ[1]); real(λ[2]) + imag(λ[2])]  # just some dummy output
    return z
end

x = [-4.0, -17.0, 2.0, 2.0, 2.5, 5.6, -4.0, 1.1]
J1 = ForwardDiff.jacobian(test, x)
J2 = ReverseDiff.jacobian(test, x)

println(J1)
println(maximum(abs.(J1 - J2)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[0.08417682040170985 0.15073775367200676 0.026880839610207964 0.04813626079392688 0.23142016831251183 0.4144104773660134 0.073901204598714 0.132336925078715; -0.040439245849026015 0.008307971974116542 -0.24954485551478056 0.05126731773490015 -0.01520651583374638 0.003124076740757695 -0.0938372543044558 0.0192782348563071]
3.469446951953614e-18</code></pre><h2 id="Ordinary-Differential-Equations"><a class="docs-heading-anchor" href="#Ordinary-Differential-Equations">Ordinary Differential Equations</a><a id="Ordinary-Differential-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Ordinary-Differential-Equations" title="Permalink"></a></h2><p>This package supports both explicit and implicit ODEs. For explicit ODEs only the reverse case is overloaded: it compiles a tape for a single time step, then reuses that tape at each subsequent time step while analytically propagating derivatives between time steps.  The precomputation and reuse of the tape can significantly reduce memory and improve speed as compared to recording the tape over the whole time series, particularly as the tape gets longer.  Forward mode is not overloaded, it just does regular forward mode AD through the whole time series, since there is nothing to exploit in an explicit case.</p><p>For implicit ODEs both reverse and forward mode are overloaded.  Reverse mode takes advantage of the same reduced tape size.  Additionally, both forward and reverse mode use implicit differentiation (direct or adjoint) for the solve that occurs at each time step.</p><p>Currently, we only support one-step methods. The underlying methodology works for multi-step methods, it is only the function signature that is currently limited to one-step.  This choice was made, at least for now, just to keep the API simple while covering the vast majority of uses cases.</p><p>Both out-of-place and in-place functions are supported.  For both ODE types, an out-of-place time step has the signature:</p><p><code>y = onestep(yprev, t, tprev, xd, xci, p)</code></p><p>whereas in-place looks like:</p><p><code>onestep!(y, yprev, t, tprev, xd, xci, p)</code></p><p>These functions update the state <code>y</code> given:</p><ul><li><code>yprev</code>: the previous state</li><li><code>t</code>: the current time</li><li><code>tprev</code>: the previous time</li><li><code>xd</code>: design variables. design variables are fixed in time but change across design iterations.</li><li><code>xci</code>: control variables. control variables change at each time step (and likely across design iterations), and <code>xci</code> refers to the control variables at this time step.</li><li><code>p</code>: parameters. parameters are fixed in time and across design iterations.</li></ul><h3 id="Explicit-ODEs"><a class="docs-heading-anchor" href="#Explicit-ODEs">Explicit ODEs</a><a id="Explicit-ODEs-1"></a><a class="docs-heading-anchor-permalink" href="#Explicit-ODEs" title="Permalink"></a></h3><p>We start with an explicit ODE, and use an out-of-place function for simplicity.  Recall that we need to pass in an ODE method with the following signature: <code>y = onestep(yprev, t, tprev, xd, xci, p)</code></p><p>In this case we choose an explicit forward Euler method.  This is not generally an effective method, but is chosen just to keep the example brief. Again, any one-step method can be used.  The unit tests show a more complex example using a 5/4 Runge-Kutta method.</p><pre><code class="language-julia hljs">using ImplicitAD
using ReverseDiff

function fwdeuler(odefun, yprev, t, tprev, xd, xci, p)

    dt = t - tprev
    y = yprev .+ dt*odefun(yprev, t, xd, xci, p)

    return y
end</code></pre><p>Note that we&#39;ve written it generically so that a user can pass in any ODE function with the signature: <code>odefun(yprev, t, xd, xci, p)</code>.  In our case, let&#39;s choose a simple set of ODEs, the Lotka–Volterra equations.  We set the variables of the Lotka–Volterra equations as design variables, so there are no parameters and no control variables:</p><pre><code class="language-julia hljs">function lotkavolterra(y, t, xd, xci, p)
    return [xd[1]*y[1] - xd[2]*y[1]*y[2];
        -xd[3]*y[2] + xd[4]*y[1]*y[2]]
end</code></pre><p>We can now combine the two to create our one-step method for this specific problem:</p><pre><code class="language-julia hljs">onestep(yprev, t, tprev, xd, xci, p) = fwdeuler(lotkavolterra, yprev, t, tprev, xd, xci, p)</code></pre><p>The package also expects an initialization function of the form: <code>y0 = initialize(t0, xd, xc0, p)</code>.  In our case, we just pass in a simple starting point:</p><pre><code class="language-julia hljs">initialize(t0, xd, xc0, p) = [1.0, 1.0]</code></pre><p>The remaining inputs are the time steps (we choose 10 seconds with a spacing of 0.1), design variables, control variables (a matrix of size number of control variables by number of time steps), and parameters (a tuple).</p><pre><code class="language-julia hljs">t = range(0.0, 10.0, step=0.1)
xd = [1.5, 1.0, 3.0, 1.0]
xc = Matrix{Float64}(undef, 0, length(t))
p = ()</code></pre><p>Let&#39;s now put this into a program.  For simplicity, let&#39;s assume that this is our entire model, it just takes the design variables in as inputs, and the overall output is the sum of our states at the last time step. If we weren&#39;t using the features of this package, we would just now initialize, then iterate through each time step to update the states.  This is a simple function to write, but there is a built-in function called <code>odesolve</code> so we&#39;ll just use it.</p><pre><code class="language-julia hljs">function program(x)
    y = ImplicitAD.odesolve(initialize, onestep, t, x, xc, p)
    return sum(y[:, end])
end</code></pre><p>However, this is not what we want.  We want to reuse the tape across time steps and analyticly propagate of derivatives between time steps, so that we don&#39;t have to record a long tape with reverse mode AD.  In this case the function just requires a name change <code>explicit_unsteady</code>:</p><pre><code class="language-julia hljs">function modprogram_nocache(x)
    y = explicit_unsteady(initialize, onestep, t, x, xc, p)
    return sum(y[:, end])
end</code></pre><p>However, to really benefit we should preallocate our array storage, and compile the reverse-mode tape.</p><div class="admonition is-warning" id="Warning-f0c41216a226a76d"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-f0c41216a226a76d" title="Permalink"></a></header><div class="admonition-body"><p>Compiling should only be done if there is no branching in your ODE (e.g., conditional statements).  Otherwise, the derivatives may not be correct since you will compile for a branch that you might not follow at a later evaluation.</p></div></div><p>In this case we can safely compile the tape.</p><pre><code class="language-julia hljs">ny = 2  # number of states
nxd = length(xd)  # number of design vars
nxc = 0  # number of control variables
cache = explicit_unsteady_cache(initialize, onestep, ny, nxd, nxc, p; compile=true)</code></pre><p>We now revise the function to use this cache.</p><pre><code class="language-julia hljs">function modprogram(x)
    y = explicit_unsteady(initialize, onestep, t, x, xc, p; cache)
    return sum(y[:, end])
end</code></pre><p>Finally, let&#39;s compute the gradients.  First, by creating a long tape with the standard approach (note that in this simple example we don&#39;t spend the effort compile this tape and preallocate the storage, but in the benchmarks shown in the paper we do this):</p><pre><code class="language-julia hljs">g1 = ReverseDiff.gradient(program, xd)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Vector{Float64}:
 -10.345407172385567
  -1.2272366146921998
   7.702314317497775
 -19.263754750014087</code></pre><p>Or by compiling across just one-time step (avoiding the memory penalties that occur for large problems) and analytically propagating between steps.</p><pre><code class="language-julia hljs">g2 = ReverseDiff.gradient(modprogram, xd)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Vector{Float64}:
 -10.345407172385567
  -1.2272366146921998
   7.702314317497775
 -19.263754750014087</code></pre><p>We can see that these give the same results.</p><pre><code class="language-julia hljs">println(g1 - g2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[0.0, 0.0, 0.0, 0.0]</code></pre><p>This problem is so small we won&#39;t see any real time difference.  The paper linked in the README shows how these benefits become significant as the number of time steps increase.</p><p>This can also be done with in-place functions.  This often requires a little more care on types, depending on the one-step method used.  The unit tests show more extensive examples, and we&#39;ll do an in-place example with implicit ODEs next.</p><h3 id="Implicit-ODEs"><a class="docs-heading-anchor" href="#Implicit-ODEs">Implicit ODEs</a><a id="Implicit-ODEs-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-ODEs" title="Permalink"></a></h3><p>Let&#39;s now try an implicit ODE time stepping method.  Here we expect to see more benefit because we can take advantage of the shorter tape compilation as in the explicit case, but we can also take advantage of adjoints applied to the solves at each time step.  We&#39;ll do this one in-place although again both out-of-place and in-place forms are accepted.</p><p>Recall that the signature for in-place is: <code>onestep!(y, yprev, t, tprev, xd, xci, p)</code>.</p><p>The only new piece of information we need for implicit methods is, like the nonlinear solver case, the residual at a given time step.  For this example, we&#39;ll solve the Robertson function that is defined as follows, written in residual form.</p><pre><code class="language-julia hljs">using ImplicitAD
using ReverseDiff
using NLsolve

function robertson(r, dy, y, t, xd, xci, p)
    r[1] = -xd[1]*y[1]               + xd[2]*y[2]*y[3] - dy[1]
    r[2] = xd[1]*y[1] - xd[3]*y[2]^2 - xd[2]*y[2]*y[3] - dy[2]
    r[3] = y[1] + y[2] + y[3] - xd[4]
end</code></pre><p>To update each state we use an implicit Euler method for simplicity:</p><pre><code class="language-julia hljs">residual!(r, y, yprev, t, tprev, xd, xci, p) = robertson(r, (y .- yprev)/(t - tprev), y, t, xd, xci, p)</code></pre><p>To solve these sets of residuals, we use the default trust-region method in NLsolve as our solver.  This now provides the expected form of our one-step update.  Note that it looks just like the explicit case since the solver occurs inside.</p><pre><code class="language-julia hljs">function onestep!(y, yprev, t, tprev, xd, xci, p)
    f!(r, yt) = residual!(r, yt, yprev, t, tprev, xd, xci, p)
    sol = nlsolve(f!, yprev, autodiff=:forward, ftol=1e-12)
    y .= sol.zero
    return nothing
end</code></pre><p>We again need an initialization function and to set the time steps, design variables, control variables, and parameters:</p><pre><code class="language-julia hljs">function initialize(t0, xd, xc0, p)
    y0 = [1.0, 0.0, 0.0]
    return y0
end

t = range(1e-6, 1e5, length=100)
xd = [0.04, 1e4, 3e7, 1.0]
xc = Matrix{Float64}(undef, 0, 100)
p = ()</code></pre><p>We again need to set the cache, but notice that it requires passing in the residual function with the signature: <code>residual!(r, y, yprev, t, tprev, xd, xci, p)</code></p><pre><code class="language-julia hljs">ny = 3
nxd = 4
nxc = 0
cache = ImplicitAD.implicit_unsteady_cache(initialize, residual!, ny, nxd, nxc, p; compile=true)</code></pre><p>Our program will return all the states at the last time step.  Again, we start with the canonical case where we don&#39;t use adjoints, and record the tape across the entire time series.</p><pre><code class="language-julia hljs">function program(x)
    y = ImplicitAD.odesolve(initialize, onestep!, t, x, xc, p)
    return y[:, end]
end</code></pre><p>The modified version only requires one new piece of information, the residuals, so that we can perform the adjoint.</p><pre><code class="language-julia hljs">function modprogram(x)
    y = implicit_unsteady(initialize, onestep!, residual!, t, x, xc, p; cache)
    return y[:, end]
end</code></pre><p>Finally, we compute the Jacobian. The original program is actually not compatible with ReverseDiff because of the internals of NLSolve.  Fortunately, the adjoint doesn&#39;t need to propagate through the solver so we use our modified version.</p><pre><code class="language-julia hljs">J = ReverseDiff.jacobian(modprogram, xd)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3×4 Matrix{Float64}:
 -0.818436     3.27073e-6   -5.45123e-10  0.0347902
 -1.50347e-6   6.00314e-12  -2.26343e-15  6.72425e-8
  0.818437    -3.27074e-6    5.45125e-10  0.96521</code></pre><p>These examples are complete but brief.  Longer examples are available in the unit tests (see <code>tests</code> folder) and in the cases from the paper (see <code>examples</code> folder).</p><h2 id="Custom-Rules-(e.g.,-calling-Python-or-other-external-code-packages)"><a class="docs-heading-anchor" href="#Custom-Rules-(e.g.,-calling-Python-or-other-external-code-packages)">Custom Rules (e.g., calling Python or other external code packages)</a><a id="Custom-Rules-(e.g.,-calling-Python-or-other-external-code-packages)-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Rules-(e.g.,-calling-Python-or-other-external-code-packages)" title="Permalink"></a></h2><p>Consider now explicit (or potentially implicit) functions of the form: <code>y = func(x, p)</code> where <code>x</code> are variables and <code>p</code> are fixed parameters.  For cases where <code>func</code> is not compatible with AD, or for cases where we have a more efficient rule, we will want to insert our own derivatives into the AD chain.  This functionality could also be used for mixed-mode AD.  For example, by wrapping some large section of code in a function that we reply reverse mode AD on, then using that as a custom rule for the overall code that might be operating in forward mode.  More complex nestings are of course possible.</p><p>One common use case for a custom rule is when an external function call is needed, i.e., a function from another programming language is used within a larger Julia code.</p><p>We provide five options for injecting the derivatives of <code>func</code>.  You can provide the Jacobian <code>J = dy/dx</code>, or the JVPs/VJPs <span>$J v$</span> and <span>$v^T J$</span>.  Alternatively, you can allow the package to estimate the derivatives using forward finite differencing, central finite differencing, or complex step.  In forward operation (with the finite differencing options) the package will choose between computing the Jacobian first or computing JVPs directly in order to minimize the number of calls to <code>func</code>.</p><p>Below is a simple example.  Let&#39;s first create a function, we call external, meant to represent a function that we cannot pass AD through (but of course can in this simple example).</p><pre><code class="language-julia hljs">function external(x, p)
    y = x.^2
    z = [x; y]
    return z
end</code></pre><p>Let&#39;s now call this function from our larger program that we wish to pass AD through:</p><pre><code class="language-julia hljs">function program(x)
    y = sin.(x)
    p = ()
    z = external(y, p)
    w = 5 * z
    return w
end</code></pre><p>Again, we assume that external is not AD compatible, so we modify this function with the <code>provide_rule</code> function provided in this package.</p><pre><code class="language-julia hljs">using ImplicitAD

function modprogram(x)
    y = sin.(x)
    p = ()
    z = provide_rule(external, y, p; mode=&quot;ffd&quot;)
    w = 5 * z
    return w
end</code></pre><p>The last argument we provided is the mode, which can be either:</p><ul><li>&quot;ffd&quot;: forward finite differencing</li><li>&quot;cfd&quot;: central finite differencing</li><li>&quot;cs&quot;: complex step</li><li>&quot;jacobian&quot;: you provide <code>J = jacobian(x, p)</code>, use also keyword jacobian</li><li>&quot;vp&quot;: you provide Jacobian vector product <code>jvp(x, p, v)</code> and vector Jacobian product <code>vjp(x, p, v)</code> see keywords <code>jvp</code> and <code>vjp</code></li></ul><p>We can now use ForwardDiff or ReverseDiff and just the external code will be finite differenced (since we chose &quot;ffd&quot; above), and inserted into the AD chain.  Since this example is actually AD compatible everywhere we compare to using ForwardDiff through everything.</p><pre><code class="language-julia hljs">using ForwardDiff
using ReverseDiff

x = [1.0; 2.0; 3.0]
Jtrue = ForwardDiff.jacobian(program, x)
J1 = ForwardDiff.jacobian(modprogram, x)
J2 = ReverseDiff.jacobian(modprogram, x)

println(Jtrue)
println(maximum(abs.(Jtrue - J1)))
println(maximum(abs.(Jtrue - J2)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[2.701511529340699 0.0 0.0; -0.0 -2.080734182735712 -0.0; -0.0 -0.0 -4.949962483002227; 4.546487134128409 0.0 0.0; -0.0 -3.7840124765396417 -0.0; -0.0 -0.0 -1.3970774909946293]
7.189802531115674e-8
7.27727562654934e-8</code></pre><p>Central difference and complex step work similarly.  The example, below shows how to provide the Jacobian.</p><pre><code class="language-julia hljs">using LinearAlgebra: diagm, I

function jacobian(x, p)
    dydx = diagm(2*x)
    dzdx = [I; dydx]
    return dzdx
end

function modprogram(x)
    y = sin.(x)
    p = ()
    z = provide_rule(external, y, p; mode=&quot;jacobian&quot;, jacobian)
    w = 5 * z
    return w
end

J1 = ForwardDiff.jacobian(modprogram, x)
J2 = ReverseDiff.jacobian(modprogram, x)
println(maximum(abs.(Jtrue - J1)))
println(maximum(abs.(Jtrue - J2)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0
0.0</code></pre><p>Finally, we show how to provide JVPs and VJPs.</p><pre><code class="language-julia hljs">function jvp(x, p, v)
    nx = length(x)
    return [v; 2*x.*v]
end

function vjp(x, p, v)
    nx = length(x)
    return v[1:nx] .+ 2*x.*v[nx+1:end]
end

function modprogram(x)
    y = sin.(x)
    p = ()
    z = provide_rule(external, y, p; mode=&quot;vp&quot;, jvp, vjp)
    w = 5 * z
    return w
end

J1 = ForwardDiff.jacobian(modprogram, x)
J2 = ReverseDiff.jacobian(modprogram, x)
println(maximum(abs.(Jtrue - J1)))
println(maximum(abs.(Jtrue - J2)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0
0.0</code></pre><div class="admonition is-info" id="Note-a8e553726bec5fd1"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-a8e553726bec5fd1" title="Permalink"></a></header><div class="admonition-body"><p>In the examples folder, there is a longer example using provide_rule with Python (PyTorch), including using the AD capabilities of PyTorch to provide the Jacobian or JVP/VJPs for ImplicitAD to use.</p></div></div><h2 id="Using-Julia-AD-to-Provide-Derivatives-in-Python"><a class="docs-heading-anchor" href="#Using-Julia-AD-to-Provide-Derivatives-in-Python">Using Julia AD to Provide Derivatives in Python</a><a id="Using-Julia-AD-to-Provide-Derivatives-in-Python-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Julia-AD-to-Provide-Derivatives-in-Python" title="Permalink"></a></h2><p>This is the opposite scenario where instead of calling functions in Python from Julia, we may want to use Julia as an AD tool for a top-level Python script.  In this example we&#39;ll first create a basic Julia function that we want to differentiate.  Note that it must have the signature <code>f = func(x, p)</code> where <code>x</code> is a vector that we want to differentiate with respect to, <code>p</code> are other input parameters (no differentiation), <code>f</code> is a vector of outputs that we differentiate.  The function can be as complex as we want, calling solvers, and using implicit differentiation or whatever.  But should be written to be Julia AD compatible.  Let&#39;s say the following function was in a file called &#39;actuator.jl&#39;.</p><pre><code class="language-juila hljs">function actuatordisk(x, p)
    a, A, rho, Vinf = x
    q = 0.5 * rho * Vinf^2
    CT = 4 * a * (1 - a)
    CP = CT * (1 - a)
    T = q * A * CT
    P = q * A * CP * Vinf
    return [T, P]
end</code></pre><p>We now want the derivatives in Python, so the below script is in Python.  Note that you must load PythonCall before ImplicitAD as the functionality shown here is designed as an optional <a href="https://pkgdocs.julialang.org/v1/creating-packages/#Conditional-loading-of-code-in-packages-(Extensions)">extension</a> to ImplicitAD.  This just means that PythonCall is not a formal dependency of ImplicitAD so that people don&#39;t need to install it.  But if they do install PythonCall then this extension is loaded in ImplicitAD for extra functionality. There is a pure Julia fallback for this same functionality, but its use cases when calling from Julia would be pretty limited as you could just use the Julia AD packages normally.  We load the file actuator.jl, though in general you could use Julia packages and could potentially even write the entire Julia script on the Python side (using juliacall syntax for all functions).</p><pre><code class="language-python hljs">import numpy as np
from juliacall import Main as jl

jl.seval(&quot;using PythonCall&quot;)
jl.seval(&quot;using ImplicitAD: derivativesetup&quot;)

jl.include(&#39;actuator.jl&#39;)

# x just needs to be representative of the size/type of x.
x = np.array([0.3, 2000.0, 1.0, 8.0])
# this should be the actual set of parameters we want to use
p = ()
# setup function to compute jacobian
jacobian = jl.derivativesetup(jl.actuatordisk, x, p, &quot;fjacobian&quot;)  # this last option means forward-mode Jacobian

# we can now reuse this function as much as we want to compute Jacobian
# the differentiation is happening on the Julia side
J = np.zeros((2, len(x)))  # we initialize J so we can populate it in-place and save storage as we recompute Jacobian

# compute Jacobian
x = np.array([0.3, 2000.0, 1.0, 8.0])
jacobian(J, x)
print(J)

# compute at some other point
x = np.array([0.25, 1800.0, 1.1, 8.5])
jacobian(J, x)
print(J)</code></pre><p>There are other options besides forward-mode Jacobian, include reverse-mode Jacobian, a Jacobian-vector product, and vector-Jacobian products.  The below example uses <a href="https://openmdao.org">OpenMDAO</a>, which can directly handle JVPs and VJPs.</p><pre><code class="language-python hljs">import numpy as np
from juliacall import Main as jl
import openmdao.api as om

jl.seval(&quot;using PythonCall&quot;)
jl.seval(&quot;using ImplicitAD: derivativesetup&quot;)


class JuliaADWrapper(om.ExplicitComponent):

    def __init__(self, func, x, nf, p):
        super().__init__()
        self.func = func
        self.x = x
        self.nf = nf
        self.p = p

    def setup(self):
        self.add_input(&#39;x&#39;, self.x)
        self.add_output(&#39;f&#39;, np.zeros(self.nf))

        self.jvp = jl.derivativesetup(self.func, self.x, self.p, &quot;jvp&quot;)
        self.vjp = jl.derivativesetup(self.func, self.x, self.p, &quot;vjp&quot;)

    def compute(self, inputs, outputs):
        outputs[&#39;f&#39;] = self.func(inputs[&#39;x&#39;], self.p)

    def compute_jacvec_product(self, inputs, d_inputs, d_outputs, mode):
        if mode == &#39;fwd&#39;:
            self.jvp(d_outputs[&#39;f&#39;], inputs[&#39;x&#39;], d_inputs[&#39;x&#39;])
        elif mode == &#39;rev&#39;:
            self.vjp(d_inputs[&#39;x&#39;], inputs[&#39;x&#39;], d_outputs[&#39;f&#39;])


if __name__ == &quot;__main__&quot;:
    jl.include(&#39;actuator.jl&#39;)
    x = np.array([0.3, 2000.0, 1.0, 8.0])
    p = ()

    prob = om.Problem()
    model = prob.model

    nf = 2
    p = ()
    model.add_subsystem(&#39;comp&#39;, JuliaADWrapper(jl.actuatordisk, x, nf, p), promotes=[&#39;*&#39;])
    prob.setup()
    model.comp.x = x
    prob.run_model()
    print(prob[&#39;f&#39;])
    # large abs error becuase derivatives are large (order 10^5) in this unscaled example
    prob.check_partials(compact_print=True, abs_err_tol=1.0, rel_err_tol=1.0E-5)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../reference/">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Monday 6 October 2025 17:15">Monday 6 October 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
