var documenterSearchIndex = {"docs":
[{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The main theory and some examples (source code available in the examples folder) are available in this paper.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Some supplementary cases that have analytic solutions (linear solvers and eigenvalues) were not shown in the paper but are described below for completeness.","category":"page"},{"location":"theory/#Linear-Equations","page":"Theory","title":"Linear Equations","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"For linear residuals the nonlinear formulation shown in the paper will of course work, but we can provide partial derivatives directly, or rather provide the solution analytically.","category":"page"},{"location":"theory/#Forward-Mode","page":"Theory","title":"Forward Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Consider the linear system: A y = b, where A and/or b is a function of our input x, and y is our state variables.  For our purposes, A and b are the inputs to our function.  The derivatives of A and b with respect to our input variables of interest (dotA and dotb) would have been computed already as part of the forward mode AD.  The solution to the linear system is represented mathematically as:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"y = A^-1 b tag1","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"and we need doty, the derivatives of y with respect to our inputs of interest.  Using the chain rule we find:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"doty = dotA^-1 b + A^-1 dotb tag2","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now need the derivative of a matrix inverse, which we can find by considering its definition relative to an identity matrix:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A A^-1 = I","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We differentiate both sides:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"dotA A^-1 + A dotA^-1 = 0","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"and we can now solve for the derivative of the matrix inverse:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nA dotA^-1 = - dotA A^-1 \n dotA^-1 = - A^-1 dotA A^-1\nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We substitute this result into Eq. (2)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"doty = - A^-1 dotA A^-1  b + A^-1 dotb","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We simplify this expression by noting that A^-1  b = y and factoring out the matrix inverse.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\ndoty = - A^-1 dotA y + A^-1 dotb\ndoty = A^-1 (-dotA y + dotb)\nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"This is the desired result:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"boxeddoty = A^-1 (dotb -dotA y)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Both dotA and dotb are already known from the previous step in the forward AD process.  We simply extract those dual numbers from the inputs.  Or more conveniently we extract the partials from the vector: (b - Ay) since y is a constant, the solution to the linear system, and thus does not contain any dual numbers. Note that we should save the factorization of A from the primal linear solve (Eq. 1) to reuse in the equation above.","category":"page"},{"location":"theory/#Reverse-Mode","page":"Theory","title":"Reverse Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Reverse mode is a bit more work.  We seek the derivatives of the overall outputs of interest with respect to A and b.  For convenience, we consider one output at a time, and again call this output xi.  Notationally this is","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA = fracd xidA barb = fracd xidb","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this step of the reverse AD chain, we would know bary, the derivative of our output of interest with respect to y.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Computing the desired derivatives is easier in index notation since it produces third order tensors (e.g., derivative of a vector with respect to a matrix).  We compute the first derivative using the chain rule","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA_ij = fracd xid A_ij = fracd xid y_k fracd y_kd A_ij = bary_k fracd y_kd A_ij\ntag3","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now need the derivative of the vector y with respect to A.  To get there, we take derivatives of our governing equation (Ay = b):","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracdd A_ij  left( A_lm y_m = b_l  right)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The vector b is independent of A; so we have:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nfracd A_lmd A_ij y_m + A_lm fracd y_md A_ij    = 0 \ndelta_lidelta_mj y_m + A_lm fracd y_mA_ij    = 0 \ndelta_li y_j + A_lm fracd y_mA_ij    = 0 \nA_nl^-1 delta_li y_j + A_nl^-1 A_lm fracd y_mA_ij    = 0 \nA_nl^-1 delta_li y_j + delta_nm fracd y_mA_ij    = 0 \nA_ni^-1  y_j +  fracd y_nA_ij    = 0\nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Thus:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracd y_kA_ij = -A_ki^-1  y_j","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now substitute this result back into Eq. (3):","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginaligned\nbarA_ij = -bary_k A_ki^-1  y_j   \nbarA_ij = - (A_ik^-Tbary_k)  y_j   \nendaligned","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"which we can recognize as an outer product","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA = - (A^-T bary) y^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now repeat the procedure for the derivatives with respect to b","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barb_i = fracd xid b_i = fracd xid y_j fracd y_jd b_i = bary_j fracd y_jd b_i","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can easily get this last derivative","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracd y_jd b_i = fracd (A^-1_jk b_k)d b_i = A^-1_jk fracd b_kd b_i = A^-1_jk delta_ki = A^-1_ji","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Thus:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barb_i = bary_j A^-1_ji","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now have the desired result","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barb = A^-T bary","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"From these results we see that we must first solve the linear system","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"boxedA^T lambda = bary","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"from which we easily get the desired derivatives % for the reverse mode pullback operation","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"    boxed\nbeginaligned\nbarA = -lambda y^T\nbarb = lambda\nendaligned\n    ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Note that we should again save the factorization for A from the primal solve to reuse in this second linear solve.","category":"page"},{"location":"theory/#Eigenvalues","page":"Theory","title":"Eigenvalues","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"A generalized eigenvalue problem solves the following equation for the pair lambda_i v_i where lambda_i is the i^textth eigenvalue and v_i the corresponding eigenvector, given the square matrices A and B:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A v_i = lambda_i B v_i\ntag4","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"For a standard eigenvalue problem, B is the identity matrix. Or in terms of left eigenvalues:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"u_i^H A = lambda_i u_i^H B\ntag5","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"For Hermitian (or real symmetric) matrices we can see that the left and right eigenvectors would be the same.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Since any eigenvector can be scaled by an arbitrary value and still satisfy the above equation, the solution is made unique by using the standard normalization:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"u_i^H B v_i = 1\ntag6","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"which we see is equivalent to u_i^H A v_i = lambda_i.","category":"page"},{"location":"theory/#Forward-Mode-2","page":"Theory","title":"Forward Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"We take derivatives of Eq. 4, using the chain rule, with respect to our inputs of interest.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"dotA v_i + A dotv_i = dotlambda_i B v_i + lambda_i dotB v_i + lambda_i B dotv_i","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We rearrange the equation collecting like terms:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"dotlambda_i B v_i  = (dotA - lambda_i dotB ) v_i + (A - lambda_i B) dotv_i","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now multiply the whole equation by u_i^H on the left:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"    dotlambda_i u_i^H B v_i  = u_i^H (dotA - lambda_i dotB ) v_i + u_i^H (A - lambda_i B) dotv_i","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"From the left eigenvalue definition (Eq. 5) the last term cancels out, and from the normalization (Eq. 6) the first term simplifies giving the desired result for the derivatives of our eigenvalues:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"    boxed\n        dotlambda_i = u_i^H (dotA - lambda_i dotB ) v_i\n    ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"As an implementation trick we can directly use the following operation: λ = λ + ui' * (A - λ * B) * vi The primal value is zero for the last term so the primal is unchanged.  For the partial term, the last term gives the desired derivatives since λ contains no dual information (only A and B contain dual numbers).","category":"page"},{"location":"theory/#Reverse-Mode-2","page":"Theory","title":"Reverse Mode","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"We follow a similar procedure to that of the linear system of equations.  Again, we consider one output at a time called xi.  The inputs are A and B and we consider only the eigenvalues as outputs lambda.  Thus, we need","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA = fracd xidA barB = fracd xidB","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"given","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barlambda = fracd xid lambda","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Using index notation we need:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"barA_ij = fracd xid A_ij = fracd xid lambda_k fracd lambda_kd A_ij\ntag7","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now need the derivative of the vector lambda with respect to A, which we get from taking derivatives of our governing equation (Eq. 4).  For simplicity, we will consider just one eigenvalue (dropping the index denoting separate eigenvalues/vectors), then will add it back in when reassembling the above equation.  The derivative of Eq. 4, in index notation, for a single eigenvalue/vector pair is:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracdd A_ij  left( A_kl v_l = lambda B_km v_m  right)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We propagate the derivative through:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"delta_ikdelta_jl v_l + A_klfracd v_ld A_ij  = fracd lambdad A_ij B_km v_m + lambda B_km fracd v_md A_ij","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The last term on each side can be combined as we recognize that l and m are both dummy indices (we will change the last term to use l instead)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"delta_ikdelta_jl v_l + (A_kl - lambda B_kl)fracd v_ld A_ij  = fracd lambdad A_ij B_km v_m","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now multiply through by the left eigenvector u_k","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"delta_ikdelta_jl u_k v_l + u_k (A_kl - lambda B_kl)fracd v_ld A_ij  = fracd lambdad A_ij u_k B_km v_m","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We recognize that the term u_k (A_kl - lambda B_kl) is zero from Eq. 5.  We also see that the term u_k B_km v_m is one from Eq. 6.  The above then simplifies to","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"delta_ikdelta_jl u_k v_l  = fracd lambdad A_ij","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We now apply the Kronecker deltas:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracd lambdad A_ij  = u_i v_j","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"or in vector notation (for a single eigenvector/value pair):","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracd lambdadA = u v^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can now compute the desired derivative from Eq. 7, where we now reinsert the summation since there are multiple eigenvalue/vectors.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"    boxedbarA = sum_k barlambda_k u_k v_k^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We repeat the same process for the derivatives with respect to B.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"beginalign*\nfracdd B_ij  ( A_kl v_l  = lambda B_km v_m  )\n  A_kl fracd v_ld B_ij  = fracd lambdad B_ij B_km v_m  + lambda fracd B_kmd B_ij v_m  + lambda B_km fracd v_md B_ij \n  (A_kl - lambda B_kl) fracd v_ld B_ij  = fracd lambdad B_ij B_km v_m  + lambda delta_ikdelta_jm v_m  \n  u_k (A_kl - lambda B_kl) fracd v_ld B_ij  = fracd lambdad B_ij u_k B_km v_m  + lambda u_k  delta_ikdelta_jm v_m  \n  0  = fracd lambdad B_ij + lambda u_k  delta_ikdelta_jm v_m  \n  fracd lambdad B_ij  =  - lambda u_i v_j\nendalign*","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"or in index notation:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"fracd lambdad B = -lambda u v^T","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We then reassemble the summation across multiple eigenvalues:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"boxedbarB = -sum_k barlambda_k lambda_k u_k v_k^T","category":"page"},{"location":"reference/#Reference","page":"API","title":"Reference","text":"","category":"section"},{"location":"reference/#ImplicitAD.implicit","page":"API","title":"ImplicitAD.implicit","text":"implicit(solve, residual, x, p=(); drdy=drdy_forward, lsolve=linear_solve)\n\nMake implicit function AD compatible (specifically with ForwardDiff and ReverseDiff).\n\nArguments\n\nsolve::function: y = solve(x, p). Solve implicit function returning state variable y, for input variables x, and fixed parameters p.\nresidual::function: Either r = residual(y, x, p) or in-place residual!(r, y, x, p). Set residual r (scalar or vector), given state y (scalar or vector), variables x (vector) and fixed parameters p (tuple).\nx::vector{float}: evaluation point.\np::tuple: fixed parameters. default is empty tuple.\ndrdy::function: drdy(residual, y, x, p).  Provide (or compute yourself): ∂ri/∂yj.  Default is forward mode AD.\nlsolve::function: lsolve(A, b).  Linear solve A x = b  (where A is computed in drdy and b is computed in jvp, or it solves A^T x = c where c is computed in vjp).  Default is backslash operator.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.implicit_linear","page":"API","title":"ImplicitAD.implicit_linear","text":"implicit_linear(A, b; lsolve=linear_solve, Af=factorize)\n\nMake implicit function AD compatible (specifically with ForwardDiff and ReverseDiff). This version is for linear equations Ay = b\n\nArguments\n\nA::matrix, b::vector: components of linear system A y = b\nlsolve::function: lsolve(A, b). Function to solve the linear system, default is backslash operator.\nAf::factorization: An optional factorization of A, useful to override default factorize, or if multiple linear solves will be performed with same A matrix.\nmmul: Function to compute A*y for a vector y. Defaults to the julia multipy operator.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.apply_factorization","page":"API","title":"ImplicitAD.apply_factorization","text":"apply_factorization(A, factfun)\n\nApply a matrix factorization to the primal portion of a dual number. Avoids user from needing to add ForwardDiff as a dependency. Afactorization = factfun(A)\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.implicit_eigval","page":"API","title":"ImplicitAD.implicit_eigval","text":"implicit_eigval(A, B, eigsolve)\n\nMake eigenvalue problems AD compatible with ForwardDiff and ReverseDiff\n\nArguments\n\nA::matrix, B::matrix: generlized eigenvalue problem. A v = λ B v  (B is identity for standard eigenvalue problem)\neigsolve::function: λ, V, U = eigsolve(A, B). Function to solve the eigenvalue problem.   λ is a vector containing eigenvalues.   V is a matrix whose columns are the corresponding eigenvectors (i.e., λ[i] corresponds to V[:, i]).   U is a matrix whose columns contain the left eigenvectors (u^H A = λ u^H B)   The left eigenvectors must be in the same order as the right ones (i.e., U' * B * V must be diagonal).   U can be provided with any normalization as we normalize internally s.t. U' * B * V = I   If A and B are symmetric/Hermitian then U = V.\n\nReturns\n\nλ::vector: eigenvalues and their derivatives.  (Currently only eigenvalue derivatives are provided.  not eigenvectors)\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.explicit_unsteady","page":"API","title":"ImplicitAD.explicit_unsteady","text":"explicit_unsteady(initialize, onestep, solve, t, xd, xc, p=(); cache=nothing)\n\nMake reverse-mode efficient for explicit ODEs Builds tape over each time step separately and analytically propagates between, rather than recording a long tape over the entire time sequence.\n\nArguments:\n\ninitialize::function: Return initial state variables.  y0 = initialize(t0, xd, xc0, p). May or may not depend   on t0 (initial time), xd (design variables), xc0 (initial control variables), p (fixed parameters)\nonestep::function: define how states are updated (assuming one-step methods). y = onestep(yprev, t, tprev, xd, xci, p)   or in-place onestep!(y, yprev, t, tprev, xd, xci, p).  Set the next set of state variables y, given the previous state yprev,   current time t, previous time tprev, design variables xd, current control variables xc, and fixed parameters p.\nt::vector{float}: time steps that simulation runs across\nxd::vector{float}: design variables, don't change in time, but do change from one run to the next (otherwise they would be parameters)\nxc::matrix{float}, size nxc x nt: control variables. xc[:, i] are the control variables at the ith time step.\np::tuple: fixed parameters, i.e., they are always constant and so do not affect derivatives.  Default is empty tuple.\n\nKeyword Arguments:\n\ncache=nothing: see explicit_unsteady_cache.  If computing derivatives more than once, you should compute the  cache beforehand the save for later iterations.  Otherwise, it will be created internally.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.explicit_unsteady_cache","page":"API","title":"ImplicitAD.explicit_unsteady_cache","text":"explicit_unsteady_cache(initialize, onestep!, ny, nxd, nxc, p=(); compile=false)\n\nInitialize arrays and functions needed for explicit_unsteady reverse mode\n\nArguments\n\ninitialize::function: Return initial state variables.  y0 = initialize(t0, xd, xc0, p). May or may not depend   on t0 (initial time), xd (design variables), xc0 (initial control variables), p (fixed parameters)\nonestep::function: define how states are updated (assuming one-step methods). y = onestep(yprev, t, tprev, xd, xci, p)   or in-place onestep!(y, yprev, t, tprev, xd, xci, p).  Set the next set of state variables y, given the previous state yprev,   current time t, previous time tprev, design variables xd, current control variables for that time step xci, and fixed parameters p.\nny::int: number of states\nnxd::int: number of design variables\nnxc::int: number of control variables (at a given time step)\np::tuple: fixed parameters, i.e., they are always constant and so do not affect derivatives.  Default is empty tuple.\ncompile::bool: indicates whether the tape for the function onestep can be  prerecorded.  Will be much faster but should only be true if onestep does not contain any branches.  Otherwise, ReverseDiff may return incorrect gradients.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.implicit_unsteady","page":"API","title":"ImplicitAD.implicit_unsteady","text":"implicit_unsteady(initialize, onestep, residual, t, xd, xc, p=(); cache=nothing, drdy=drdy_forward, lsolve=linear_solve)\n\nMake reverse-mode efficient for implicit ODEs (solvers compatible with AD).\n\nArguments:\n\ninitialize::function: Return initial state variables.  y0 = initialize(t0, xd, xc0, p). May or may not depend   on t0 (initial time), xd (design variables), xc0 (initial control variables), p (fixed parameters)\nonestep::function: define how states are updated (assuming one-step methods) including any solver. y = onestep(yprev, t, tprev, xd, xci, p)   or in-place onestep!(y, yprev, t, tprev, xd, xci, p).  Set the next set of state variables y, given the previous state yprev,   current time t, previous time tprev, design variables xd, current control variables for that time step xci, and fixed parameters p.\nresidual::function: define residual that is solved in onestep.  Either r = residual(y, yprev, t, tprev, xd, xci, p) where variables are same as above or   residual!(r, y, yprev, t, tprev, xd, xci, p).\nt::vector{float}: time steps that simulation runs across\nxd::vector{float}: design variables, don't change in time, but do change from one run to the next (otherwise they would be parameters)\nxc::matrix{float}, size nxc x nt: control variables. xc[:, i] are the control variables at the ith time step.\np::tuple: fixed parameters, i.e., they are always constant and so do not affect derivatives.  Default is empty tuple.\n\nKeyword Arguments:\n\ncache=nothing: see implicit_unsteady_cache.  If computing derivatives more than once, you should compute the   cache beforehand the save for later iterations.  Otherwise, it will be created internally.\ndrdy: drdy(residual, r, y, yprev, t, tprev, xd, xci, p). Provide (or compute yourself): ∂ri/∂yj.  Default is   forward mode AD.\nlsolve::function: lsolve(A, b). Linear solve A x = b (where A is computed in  drdy and b is computed in jvp, or it solves A^T x = c where c is computed  in vjp). Default is backslash operator.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.implicit_unsteady_cache","page":"API","title":"ImplicitAD.implicit_unsteady_cache","text":"implicit_unsteady_cache(initialize, residual, ny, nxd, nxc, p=(); compile=false)\n\nInitialize arrays and functions needed for implicit_unsteady reverse mode\n\nArguments\n\ninitialize::function: Return initial state variables.  y0 = initialize(t0, xd, xc0, p). May or may not depend   on t0 (initial time), xd (design variables), xc0 (initial control variables), p (fixed parameters)\nresidual::function: define residual that is solved in onestep.  Either r = residual(y, yprev, t, tprev, xd, xci, p) where variables are same as above   residual!(r, y, yprev, t, tprev, xd, xci, p).\nny::int: number of states\nnxd::int: number of design variables\nnxc::int: number of control variables (at a given time step)\np::tuple: fixed parameters, i.e., they are always constant and so do not affect derivatives.  Default is empty tuple.\ncompile::bool: indicates whether the tape for the function onestep can be  prerecorded.  Will be much faster but should only be true if onestep does not contain any branches.  Otherwise, ReverseDiff may return incorrect gradients.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ImplicitAD.provide_rule","page":"API","title":"ImplicitAD.provide_rule","text":"provide_rule(func, x, p=(); mode=\"ffd\", jacobian=nothing, jvp=nothing, vjp=nothing)\n\nProvide partials rather than rely on AD.  For cases where AD is not available or to provide your own rule, or to use mixed mode, etc.\n\nArguments\n\nfunc::function, x::vector{float}, p::tuple:  function of form: y = func(x, p), where x are variables and p are fixed parameters.\nmode::string:\n\"ffd\": forward finite difference\n\"cfd\": central finite difference\n\"cs\": complex step\n\"jacobian\": provide your own jacobian (see jacobian function)\n\"vp\": provide jacobian vector product and vector jacobian product (see jvp and vjp)\njacobian::function: only used if mode=\"jacobian\". J = jacobian(x, p) provide Jij = dyi / dx_j\njvp::function: only used if mode=\"vp\" and in forward mode. ydot = jvp(x, p, v) provide Jacobian vector product J*v\nvjp::function: only used if mode=\"vp\" and in revese mode. xbar = vjp(x, p, v) provide vector Jacobian product v'*J\n\n\n\n\n\n","category":"function"},{"location":"#ImplicitAD-Documentation","page":"Home","title":"ImplicitAD Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Summary: Automate steady and unsteady adjoints.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Make implicit functions compatible with algorithmic differentiation (AD) without differentiating inside the solvers (discrete adjoint). Even though one can sometimes propagate AD through a solver, this is typically inefficient and less accurate.  Instead, one should use adjoints or direct (forward) methods. However, implementing adjoints is often cumbersome. This package allows for a one-line change to automate this process.  End-users can then use your package with AD normally, and utilize adjoints automatically.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We've also enabled methods to efficiently compute derivatives through explicit and implicit ODE solvers (unsteady discrete adjoint).  For the implicit solve at each time step we can apply the same methodology.  However, both still face memory challenges for long time-based simulations.  We analytically propagate derivatives between time steps so that reverse mode AD tapes only need to extend across a single time step. This allows for arbitrarily long time sequences without increasing memory requirements.","category":"page"},{"location":"","page":"Home","title":"Home","text":"As a side benefit the above functionality easily allows one to define custom AD rules.  This is perhaps most useful when calling code from another language.  We provide fall backs for utilizing finite differencing and complex step efficiently if the external code cannot provide derivatives (ideally via Jacobian vector products).  This functionality can also be used for mixed-mode AD.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Author: Andrew Ning and Taylor McDonnell","category":"page"},{"location":"","page":"Home","title":"Home","text":"Features:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Compatible with ForwardDiff and ReverseDiff (or any ChainRules compliant reverse mode AD package)\nCompatible with any solver (no differentiation occurs inside the solver)\nSimple drop in functionality\nCustomizable subfunctions to accommodate different use cases (e.g., custom linear solvers, factorizations, matrix-free operators)\nVersion for ordinary differentiation equations (i.e., discrete unsteady adjoint)\nAnalytic overrides for linear systems (more efficient)\nAnalytic overrides for eigenvalue problems (more efficient)\nCan provide custom rules to be inserted into the AD chain (e.g., interfacing with python).  Provides finite differencing and complex step defaults for cases where AD is not available (e.g., calling another language).  But can also provide Jacobians or JVP/VJPs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Start with the tutorial to learn usage.\nThe API is described in the reference page.\nThe math is particularly helpful for those wanting to provide their own custom subfunctions. See the theory and also some scaling examples in this PDF.  A supplementary document deriving the linear and eigenvalue cases is available in the theory section.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Run Unit Tests:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> activate .\npkg> test","category":"page"},{"location":"","page":"Home","title":"Home","text":"Citing:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please cite the following preprint.  DOI: 10.48550/arXiv.2306.15243","category":"page"},{"location":"","page":"Home","title":"Home","text":"Other Packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nonconvex.jl and ImplicitDifferentiation.jl are other implementations of the nonlinear portion of this package.  SciML provides support for continuous unsteady adjoints of ODEs.  They have also recently added an implementation for the nonlinear case.","category":"page"},{"location":"tutorial/#Quick-Start","page":"Tutorial","title":"Quick Start","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can generically represent the solver that converges the residuals and computes the corresponding state variables as:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"y = solve(x, p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Where x are variables and p are fixed parameters.  Our larger code may then have a mix of explicit and implicit functions.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function example(a)\n    b = 2*a\n    x = @. exp(b) + a\n    y = solve(x)\n    z = sqrt.(y)\n    return z\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To make this function compatible we only need to replace the call to solve with an overloaded function implicit defined in this module:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ImplicitAD\n\nfunction example(a)\n    b = 2*a\n    x = @. exp(b) + a\n    y = implicit(solve, residual, x)\n    z = sqrt.(y)\n    return z\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that the only new piece of information we need to expose is the residual function so that partial derivatives can be computed.  The new function is now compatible with ForwardDiff or ReverseDiff, for any solver, and efficiently provides the correct derivatives without differentiating inside the solver.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The residuals can either be returned: r = residual(y, x, p) or modified in place: residual!(r, y, x, p).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The input x should be a vector, and p is a tuple of fixed parameters.  The state and corresponding residuals, y and r, can be vectors or scalars (for 1D residuals).  The subfunctions are overloaded to handle both cases efficiently.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Limitation: ReverseDiff does not currently support compiling the tape for custome rules.  See this issue in ReverseDiff: https://github.com/JuliaDiff/ReverseDiff.jl/issues/187","category":"page"},{"location":"tutorial/#Basic-Usage","page":"Tutorial","title":"Basic Usage","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's go through a complete example now. Assume we have two nonlinear implicit equations:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"r_1(x y) = (y_1 + x_1) (y_2^3 - x_2) + x_3 = 0\nr_2(x y) = sin(y_2 exp(y_1) - 1) x_4 = 0","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We will use the NLsolve package to solve these equations (refer to the first example in their documentation if not familiar with NLsolve).  We will also put explict operations before and after the solve just to show how this will work in the midst of a larger program.  In this case we use the in-place form of the residual function.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using NLsolve\n\nfunction residual!(r, y, x, p)\n    r[1] = (y[1] + x[1])*(y[2]^3-x[2])+x[3]\n    r[2] = sin(y[2]*exp(y[1])-1)*x[4]\nend\n\nfunction solve(x, p)\n    rwrap(r, y) = residual!(r, y, x[1:4], p)  # closure using some of the input variables within x just as an example\n    res = nlsolve(rwrap, [0.1; 1.2], autodiff=:forward)\n    return res.zero\nend\n\nfunction program(x)\n    z = 2.0*x\n    w = z + x.^2\n    y = solve(w)\n    return y[1] .+ w*y[2]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now if we tried to run ForwardDiff.jacobian(program, x) it will not work (actually it will work if we change the type of our starting point x_0 to also be a Dual).  But even for solvers where we can propagate AD through it would typically be an inefficient way to compute the derivatives.  We now need to modify this script to use our package.  Here is what the modified program function will look like.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ImplicitAD\n\nfunction modprogram(x)\n    z = 2.0*x\n    w = z + x.^2\n    y = implicit(solve, residual!, w)\n    return y[1] .+ w*y[2]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"It is now both ForwardDiff and ReverseDiff compatible.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ForwardDiff\nusing ReverseDiff\n\nx = [1.0; 2.0; 3.0; 4.0; 5.0]\n\nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\nprintln(J1)\nprintln(\"max abs difference = \", maximum(abs.(J1 - J2)))","category":"page"},{"location":"tutorial/#Overloading-Subfunctions","page":"Tutorial","title":"Overloading Subfunctions","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If the user can provide (or lazily compute) their own partial derivatives for ∂r/∂y then they can provide their own subfunction: ∂r∂y = drdy(residual, y, x, p) (where r = residual(y, x, p)).  The default implementation computes these partials with ForwardDiff. Additionally the user can override the linear solve: x = lsolve(A, b).  The default is the backslash operator.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Some examples where one may wish to override these behaviors are for cases with significant sparsity (e.g., using SparseDiffTools), to preallocate the Jacobian, to provide a specific matrix factorization, or if the number of states is large overriding both methods will often be beneficial so that you can use iterative linear solvers (matrix-free Krylov methods) and thus provide efficient Jacobian vector products rather than a Jacobian.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The other partials, ∂r/∂x, are not computed directly, but rather are used in efficient Jacobian vector (or vector Jacobian) products.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As an example, let's continue the same problem from the previous section.  We note that we can provide the Jacobian ∂r/∂y analytically and so we will skip the internal ForwardDiff implementation. We provide our own function for drdy, and we will preallocate so we can modify the Jacobian in place:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function drdy(residual, y, x, p, A)\n    A[1, 1] = y[2]^3-x[2]\n    A[1, 2] = 3*y[2]^2*(y[1]+x[1])\n    u = exp(y[1])*cos(y[2]*exp(y[1])-1)*x[4]\n    A[2, 1] = y[2]*u\n    A[2, 2] = u\n    return A\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can now pass this function in with a keyword argument to replace the default implementation for this subfunction.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function modprogram2(x)\n    z = 2.0*x\n    w = z + x.^2\n    A = zeros(2, 2)\n    my_drdy(residual, y, x, p) = drdy(residual, y, x, p, A)\n    p = () # no parameters in this case\n    y = implicit(solve, residual!, w, p, drdy=my_drdy)\n    return y[1] .+ w*y[2]\nend\n\nJ3 = ForwardDiff.jacobian(modprogram2, x)\nprintln(maximum(abs.(J1 - J3)))\n","category":"page"},{"location":"tutorial/#Linear-residuals","page":"Tutorial","title":"Linear residuals","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If the residuals are linear (i.e., Ay = b) we could still use the above nonlinear formulation but it will be inefficient or require more work from the user.  Instead, we can provide the partial derivatives analytically for the user.  In this case, the user need only provide the inputs A and b.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's consider a simple example.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using SparseArrays: sparse\n\nfunction program(x)\n    Araw = [x[1]*x[2] x[3]+x[4];\n        x[3]+x[4] 0.0]\n    b = [2.0, 3.0]\n    A = sparse(Araw)\n    y = A \\ b\n    z = y.^2\n    return z\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This function is actually not compatible with ForwardDiff because of the use of a sparse matrix (obviously unnecessary with such a small matrix, just for illustration).  We now modify this function using this package, with a one line change using implicit_linear, and can now compute the Jacobian.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's consider a simple example.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ImplicitAD\nusing ForwardDiff\nusing ReverseDiff\n\nfunction modprogram(x)\n    Araw = [x[1]*x[2] x[3]+x[4];\n        x[3]+x[4] 0.0]\n    b = [2.0, 3.0]\n    A = sparse(Araw)\n    y = implicit_linear(A, b)\n    z = y.^2\n    return z\nend\n\nx = [1.0; 2.0; 3.0; 4.0]\n\nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\n\nprintln(J1)\nprintln(maximum(abs.(J1 - J2)))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For implicit_linear there are two keywords for custom subfunctions:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lsolve(A, b): same purpose as before: solve A x = b where the default is the backslash operator.\nfact(A): provide a matrix factorization of A, since two linear solves are performed (for the primal and dual values).  default is factorize defined in LinearAlgebra.","category":"page"},{"location":"tutorial/#Eigenvalue-Problems","page":"Tutorial","title":"Eigenvalue Problems","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Like the linear case, we can provide analytic derivatives for eigenvalue problems (many of which are not overridden for AD anyway).  These are problems of the form:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"A v = lambda B v","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For standard eigenvalue problems B is the identity matrix.  The user just needs to provide the matrices A, B, and some function to solve the eigenvalue problem (which could use any method).  The solver should be in the following form: λ, V, U = eigsolve(A, B) where λ is a vector of eigenvalues, V is a matrix with corresponding eigenvectors in the columns (i.e., λ[i] corresponds to V[:, i]), and U is a matrix whose columns contain the left eigenvectors (u^H A = λ u^H B).  The left eigenvectors must be in the same order as the right eigenvectors (i.e., U' * B * V must be diagonal).  U need not be normalized as we do that internally.  Note that if A and B are symmetric/Hermitian then U = V.  Currently only eigenvalue derivatives are provided (not eigenvector derivatives). Let's now see an example.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ImplicitAD\nusing ForwardDiff\nusing ReverseDiff\nusing LinearAlgebra: eigvals, eigvecs\n\nfunction eigsolve(A, B)\n    λ = eigvals(A, B)\n    V = eigvecs(A, B)\n    U = eigvecs(A', B')\n\n    return λ, V, U\nend\n\nfunction test(x)\n    A = [x[1] x[2]; x[3] x[4]]\n    B = [x[5] x[6]; x[7] x[8]]\n    λ = ImplicitAD.implicit_eigval(A, B, eigsolve)  # replaced from λ, _, _ = eigsolve(A, B)\n    z = [real(λ[1]) + imag(λ[1]); real(λ[2]) + imag(λ[2])]  # just some dummy output\n    return z\nend\n\nx = [-4.0, -17.0, 2.0, 2.0, 2.5, 5.6, -4.0, 1.1]\nJ1 = ForwardDiff.jacobian(test, x)\nJ2 = ReverseDiff.jacobian(test, x)\n\nprintln(J1)\nprintln(maximum(abs.(J1 - J2)))","category":"page"},{"location":"tutorial/#Ordinary-Differential-Equations","page":"Tutorial","title":"Ordinary Differential Equations","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This package supports both explicit and implicit ODEs. For explicit ODEs only the reverse case is overloaded: it compiles a tape for a single time step, then reuses that tape at each subsequent time step while analytically propagating derivatives between time steps.  The precomputation and reuse of the tape can significantly reduce memory and improve speed as compared to recording the tape over the whole time series, particularly as the tape gets longer.  Forward mode is not overloaded, it just does regular forward mode AD through the whole time series, since there is nothing to exploit in an explicit case.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For implicit ODEs both reverse and forward mode are overloaded.  Reverse mode takes advantage of the same reduced tape size.  Additionally, both forward and reverse mode use implicit differentiation (direct or adjoint) for the solve that occurs at each time step.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Currently, we only support one-step methods. The underlying methodology works for multi-step methods, it is only the function signature that is currently limited to one-step.  This choice was made, at least for now, just to keep the API simple while covering the vast majority of uses cases.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Both out-of-place and in-place functions are supported.  For both ODE types, an out-of-place time step has the signature:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"y = onestep(yprev, t, tprev, xd, xci, p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"whereas in-place looks like:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"onestep!(y, yprev, t, tprev, xd, xci, p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"These functions update the state y given:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"yprev: the previous state\nt: the current time\ntprev: the previous time\nxd: design variables. design variables are fixed in time but change across design iterations.\nxci: control variables. control variables change at each time step (and likely across design iterations), and xci refers to the control variables at this time step.\np: parameters. parameters are fixed in time and across design iterations.","category":"page"},{"location":"tutorial/#Explicit-ODEs","page":"Tutorial","title":"Explicit ODEs","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We start with an explicit ODE, and use an out-of-place function for simplicity.  Recall that we need to pass in an ODE method with the following signature: y = onestep(yprev, t, tprev, xd, xci, p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this case we choose an explicit forward Euler method.  This is not generally an effective method, but is chosen just to keep the example brief. Again, any one-step method can be used.  The unit tests show a more complex example using a 5/4 Runge-Kutta method.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ImplicitAD\nusing ReverseDiff\n\nfunction fwdeuler(odefun, yprev, t, tprev, xd, xci, p)\n\n    dt = t - tprev\n    y = yprev .+ dt*odefun(yprev, t, xd, xci, p)\n\n    return y\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that we've written it generically so that a user can pass in any ODE function with the signature: odefun(yprev, t, xd, xci, p).  In our case, let's choose a simple set of ODEs, the Lotka–Volterra equations.  We set the variables of the Lotka–Volterra equations as design variables, so there are no parameters and no control variables:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function lotkavolterra(y, t, xd, xci, p)\n    return [xd[1]*y[1] - xd[2]*y[1]*y[2];\n        -xd[3]*y[2] + xd[4]*y[1]*y[2]]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can now combine the two to create our one-step method for this specific problem:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"onestep(yprev, t, tprev, xd, xci, p) = fwdeuler(lotkavolterra, yprev, t, tprev, xd, xci, p)\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The package also expects an initialization function of the form: y0 = initialize(t0, xd, xc0, p).  In our case, we just pass in a simple starting point:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"initialize(t0, xd, xc0, p) = [1.0, 1.0]\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The remaining inputs are the time steps (we choose 10 seconds with a spacing of 0.1), design variables, control variables (a matrix of size number of control variables by number of time steps), and parameters (a tuple).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"t = range(0.0, 10.0, step=0.1)\nxd = [1.5, 1.0, 3.0, 1.0]\nxc = Matrix{Float64}(undef, 0, length(t))\np = ()\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's now put this into a program.  For simplicity, let's assume that this is our entire model, it just takes the design variables in as inputs, and the overall output is the sum of our states at the last time step. If we weren't using the features of this package, we would just now initialize, then iterate through each time step to update the states.  This is a simple function to write, but there is a built-in function called odesolve so we'll just use it.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function program(x)\n    y = ImplicitAD.odesolve(initialize, onestep, t, x, xc, p)\n    return sum(y[:, end])\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"However, this is not what we want.  We want to reuse the tape across time steps and analyticly propagate of derivatives between time steps, so that we don't have to record a long tape with reverse mode AD.  In this case the function just requires a name change explicit_unsteady:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function modprogram_nocache(x)\n    y = explicit_unsteady(initialize, onestep, t, x, xc, p)\n    return sum(y[:, end])\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"However, to really benefit we should preallocate our array storage, and compile the reverse-mode tape.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"warning: Warning\nCompiling should only be done if there is no branching in your ODE (e.g., conditional statements).  Otherwise, the derivatives may not be correct since you will compile for a branch that you might not follow at a later evaluation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this case we can safely compile the tape.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"ny = 2  # number of states\nnxd = length(xd)  # number of design vars\nnxc = 0  # number of control variables\ncache = explicit_unsteady_cache(initialize, onestep, ny, nxd, nxc, p; compile=true)\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We now revise the function to use this cache.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function modprogram(x)\n    y = explicit_unsteady(initialize, onestep, t, x, xc, p; cache)\n    return sum(y[:, end])\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, let's compute the gradients.  First, by creating a long tape with the standard approach (note that in this simple example we don't spend the effort compile this tape and preallocate the storage, but in the benchmarks shown in the paper we do this):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"g1 = ReverseDiff.gradient(program, xd)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Or by compiling across just one-time step (avoiding the memory penalties that occur for large problems) and analytically propagating between steps.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"g2 = ReverseDiff.gradient(modprogram, xd)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can see that these give the same results.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"println(g1 - g2)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This problem is so small we won't see any real time difference.  The paper linked in the README shows how these benefits become significant as the number of time steps increase.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This can also be done with in-place functions.  This often requires a little more care on types, depending on the one-step method used.  The unit tests show more extensive examples, and we'll do an in-place example with implicit ODEs next.","category":"page"},{"location":"tutorial/#Implicit-ODEs","page":"Tutorial","title":"Implicit ODEs","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's now try an implicit ODE time stepping method.  Here we expect to see more benefit because we can take advantage of the shorter tape compilation as in the explicit case, but we can also take advantage of adjoints applied to the solves at each time step.  We'll do this one in-place although again both out-of-place and in-place forms are accepted.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Recall that the signature for in-place is: onestep!(y, yprev, t, tprev, xd, xci, p).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The only new piece of information we need for implicit methods is, like the nonlinear solver case, the residual at a given time step.  For this example, we'll solve the Robertson function that is defined as follows, written in residual form.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ImplicitAD\nusing ReverseDiff\nusing NLsolve\n\nfunction robertson(r, dy, y, t, xd, xci, p)\n    r[1] = -xd[1]*y[1]               + xd[2]*y[2]*y[3] - dy[1]\n    r[2] = xd[1]*y[1] - xd[3]*y[2]^2 - xd[2]*y[2]*y[3] - dy[2]\n    r[3] = y[1] + y[2] + y[3] - xd[4]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To update each state we use an implicit Euler method for simplicity:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"residual!(r, y, yprev, t, tprev, xd, xci, p) = robertson(r, (y .- yprev)/(t - tprev), y, t, xd, xci, p)\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To solve these sets of residuals, we use the default trust-region method in NLsolve as our solver.  This now provides the expected form of our one-step update.  Note that it looks just like the explicit case since the solver occurs inside.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function onestep!(y, yprev, t, tprev, xd, xci, p)\n    f!(r, yt) = residual!(r, yt, yprev, t, tprev, xd, xci, p)\n    sol = nlsolve(f!, yprev, autodiff=:forward, ftol=1e-12)\n    y .= sol.zero\n    return nothing\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We again need an initialization function and to set the time steps, design variables, control variables, and parameters:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function initialize(t0, xd, xc0, p)\n    y0 = [1.0, 0.0, 0.0]\n    return y0\nend\n\nt = range(1e-6, 1e5, length=100)\nxd = [0.04, 1e4, 3e7, 1.0]\nxc = Matrix{Float64}(undef, 0, 100)\np = ()\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We again need to set the cache, but notice that it requires passing in the residual function with the signature: residual!(r, y, yprev, t, tprev, xd, xci, p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"ny = 3\nnxd = 4\nnxc = 0\ncache = ImplicitAD.implicit_unsteady_cache(initialize, residual!, ny, nxd, nxc, p; compile=true)\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Our program will return all the states at the last time step.  Again, we start with the canonical case where we don't use adjoints, and record the tape across the entire time series.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function program(x)\n    y = ImplicitAD.odesolve(initialize, onestep!, t, x, xc, p)\n    return y[:, end]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The modified version only requires one new piece of information, the residuals, so that we can perform the adjoint.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function modprogram(x)\n    y = implicit_unsteady(initialize, onestep!, residual!, t, x, xc, p; cache)\n    return y[:, end]\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, we compute the Jacobian. The original program is actually not compatible with ReverseDiff because of the internals of NLSolve.  Fortunately, the adjoint doesn't need to propagate through the solver so we use our modified version.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"J = ReverseDiff.jacobian(modprogram, xd)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"These examples are complete but brief.  Longer examples are available in the unit tests (see tests folder) and in the cases from the paper (see examples folder).","category":"page"},{"location":"tutorial/#Custom-Rules","page":"Tutorial","title":"Custom Rules","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Consider now explicit (or potentially implicit) functions of the form: y = func(x, p) where x are variables and p are fixed parameters.  For cases where func is not compatible with AD, or for cases where we have a more efficient rule, we will want to insert our own derivatives into the AD chain.  This functionality could also be used for mixed-mode AD.  For example, by wrapping some large section of code in a function that we reply reverse mode AD on, then using that as a custom rule for the overall code that might be operating in forward mode.  More complex nestings are of course possible.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"One common use case for a custom rule is when an external function call is needed, i.e., a function from another programming language is used within a larger Julia code.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We provide five options for injecting the derivatives of func.  You can provide the Jacobian J = dy/dx, or the JVPs/VJPs J v and v^T J.  Alternatively, you can allow the package to estimate the derivatives using forward finite differencing, central finite differencing, or complex step.  In forward operation (with the finite differencing options) the package will choose between computing the Jacobian first or computing JVPs directly in order to minimize the number of calls to func.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Below is a simple example.  Let's first create a function, we call external, meant to represent a function that we cannot pass AD through (but of course can in this simple example).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function external(x, p)\n    y = x.^2\n    z = [x; y]\n    return z\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's now call this function from our larger program that we wish to pass AD through:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function program(x)\n    y = sin.(x)\n    p = ()\n    z = external(y, p)\n    w = 5 * z\n    return w\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Again, we assume that external is not AD compatible, so we modify this function with the provide_rule function provided in this package.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ImplicitAD\n\nfunction modprogram(x)\n    y = sin.(x)\n    p = ()\n    z = provide_rule(external, y, p; mode=\"ffd\")\n    w = 5 * z\n    return w\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The last argument we provided is the mode, which can be either:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"\"ffd\": forward finite differencing\n\"cfd\": central finite differencing\n\"cs\": complex step\n\"jacobian\": you provide J = jacobian(x, p), use also keyword jacobian\n\"vp\": you provide Jacobian vector product jvp(x, p, v) and vector Jacobian product vjp(x, p, v) see keywords jvp and vjp","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can now use ForwardDiff or ReverseDiff and just the external code will be finite differenced (since we chose \"ffd\" above), and inserted into the AD chain.  Since this example is actually AD compatible everywhere we compare to using ForwardDiff through everything.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ForwardDiff\nusing ReverseDiff\n\nx = [1.0; 2.0; 3.0]\nJtrue = ForwardDiff.jacobian(program, x)\nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\n\nprintln(Jtrue)\nprintln(maximum(abs.(Jtrue - J1)))\nprintln(maximum(abs.(Jtrue - J2)))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Central difference and complex step work similarly.  The example, below shows how to provide the Jacobian.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra: diagm, I\n\nfunction jacobian(x, p)\n    dydx = diagm(2*x)\n    dzdx = [I; dydx]\n    return dzdx\nend\n\nfunction modprogram(x)\n    y = sin.(x)\n    p = ()\n    z = provide_rule(external, y, p; mode=\"jacobian\", jacobian)\n    w = 5 * z\n    return w\nend\n\nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\nprintln(maximum(abs.(Jtrue - J1)))\nprintln(maximum(abs.(Jtrue - J2)))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, we show how to provide JVPs and VJPs.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"\nfunction jvp(x, p, v)\n    nx = length(x)\n    return [v; 2*x.*v]\nend\n\nfunction vjp(x, p, v)\n    nx = length(x)\n    return v[1:nx] .+ 2*x.*v[nx+1:end]\nend\n\nfunction modprogram(x)\n    y = sin.(x)\n    p = ()\n    z = provide_rule(external, y, p; mode=\"vp\", jvp, vjp)\n    w = 5 * z\n    return w\nend\n\nJ1 = ForwardDiff.jacobian(modprogram, x)\nJ2 = ReverseDiff.jacobian(modprogram, x)\nprintln(maximum(abs.(Jtrue - J1)))\nprintln(maximum(abs.(Jtrue - J2)))","category":"page"}]
}
